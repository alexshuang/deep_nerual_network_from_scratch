{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "HeadFirstRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "54DSp8j5gJy8",
        "5ulBMFjZgJzF",
        "i9_28v_8gJzK",
        "5iS4iqWKgJzR"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBOEit_GgJwM",
        "colab_type": "text"
      },
      "source": [
        "# Predicting English word version of numbers using an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxQ30yEIgJwR",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luW02YOTgJwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.text import *\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5_sNlsqgJwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5nN8YwvgJwY",
        "colab_type": "code",
        "outputId": "07607239-39ab-4bed-b291-60f34efdfb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "path = untar_data(URLs.HUMAN_NUMBERS)\n",
        "path.ls()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://files.fast.ai/data/examples/human_numbers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/human_numbers/valid.txt'),\n",
              " PosixPath('/root/.fastai/data/human_numbers/train.txt')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf4-a67lgJwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readnums(d): return [', '.join(o.strip() for o in open(path/d).readlines())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec0cKxTGgJwf",
        "colab_type": "text"
      },
      "source": [
        "train.txt gives us a sequence of numbers written out as English words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GscWnmuKgJwg",
        "colab_type": "code",
        "outputId": "fd550bc4-9edb-4483-8af8-65b0113b894a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_txt = readnums('train.txt'); train_txt[0][:80]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdt2u_Q_gJwj",
        "colab_type": "code",
        "outputId": "8ba08f94-dc5c-42b7-effd-391d52f30cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "valid_txt = readnums('valid.txt'); valid_txt[0][-80:]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' nine thousand nine hundred ninety eight, nine thousand nine hundred ninety nine'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMpUPMTdgdDW",
        "colab_type": "code",
        "outputId": "b68b7324-ba6b-4d82-b199-44cc96a6f95e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "train = TextList(train_txt, path=path)\n",
        "valid = TextList(valid_txt, path=path)\n",
        "src = ItemLists(path, train, valid).label_for_lm()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cElyfmSpgJyK",
        "colab_type": "text"
      },
      "source": [
        "## Single output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1TtThwhbYct",
        "colab_type": "text"
      },
      "source": [
        "N-Gram, study 20 tokens, then predict the 21th."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxM8AtR8gJyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_f(input, target): return F.cross_entropy(input, target[:,-1])\n",
        "def acc_f(input, target): return accuracy(input, target[:,-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcM9Hcf0lJvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordvec_len = 100\n",
        "nh = 64\n",
        "nv = len(data.train_ds.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZrLJjb6gJwz",
        "colab_type": "text"
      },
      "source": [
        "`bptt` stands for *back-propagation through time*.  This tells us how many steps of history we are considering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeDmUgmNX-mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bptt = 3\n",
        "data = src.databunch(bs=bs, bptt=bptt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aEYml4XgJxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = data.train_ds.vocab\n",
        "nv = len(v.itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o_X-3sux2Md",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(nv, wordvec_len) # word2vec\n",
        "    self.input = nn.Linear(wordvec_len, nh) # input layer, green arrors\n",
        "    self.hid = nn.Linear(nh, nh) # hidden layer, orange arrors\n",
        "    self.out = nn.Linear(nh, nv) # output layer, green arrors\n",
        "    self.bn = nn.BatchNorm1d(nh)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = torch.zeros(x.shape[0], nh).to(device=x.device)\n",
        "    for i in range(x.shape[1]):\n",
        "      h = h + F.relu(self.input(self.emb(x[:, i])))\n",
        "      h = self.bn(F.relu(self.hid(h)))\n",
        "    return self.out(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZCXA4DG338Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "4e2c5533-e504-4bb5-c895-49659d125079"
      },
      "source": [
        "learn = Learner(data, Model1(), loss_func=loss_f, metrics=acc_f)\n",
        "learn.fit_one_cycle(10, 1e-4)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>acc_f</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.546392</td>\n",
              "      <td>3.752568</td>\n",
              "      <td>0.024586</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.911883</td>\n",
              "      <td>3.326832</td>\n",
              "      <td>0.227022</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.289827</td>\n",
              "      <td>2.718843</td>\n",
              "      <td>0.447610</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.943045</td>\n",
              "      <td>2.349009</td>\n",
              "      <td>0.465303</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.766844</td>\n",
              "      <td>2.192134</td>\n",
              "      <td>0.466222</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.678379</td>\n",
              "      <td>2.125904</td>\n",
              "      <td>0.466452</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.633395</td>\n",
              "      <td>2.100061</td>\n",
              "      <td>0.464844</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.610903</td>\n",
              "      <td>2.090777</td>\n",
              "      <td>0.464614</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.601126</td>\n",
              "      <td>2.087984</td>\n",
              "      <td>0.459099</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.598013</td>\n",
              "      <td>2.088171</td>\n",
              "      <td>0.459099</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi6YD0sugJyy",
        "colab_type": "text"
      },
      "source": [
        "## Multi output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaAsLwq0gJyy",
        "colab_type": "text"
      },
      "source": [
        "Before, we were just predicting the last word in a line of text.  Given 70 tokens, what is token 71?  That approach was throwing away a lot of data.  Why not predict token 2 from token 1, then predict token 3, then predict token 4, and so on?  We will modify our model to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq3N6OdjcpyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bptt = 20\n",
        "data = src.databunch(bs=bs, bptt=bptt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9jNlbpy4j3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(nv, wordvec_len)\n",
        "    self.input = nn.Linear(wordvec_len, nh)\n",
        "    self.hid = nn.Linear(nh, nh)\n",
        "    self.out = nn.Linear(nh, nv)\n",
        "    self.bn = nn.BatchNorm1d(nh)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = torch.zeros(x.shape[0], nh).to(device=x.device)\n",
        "    res = []\n",
        "    for i in range(x.shape[1]):\n",
        "      h = h + F.relu(self.input(self.emb(x[:, i])))\n",
        "      h = self.bn(F.relu(self.hid(h)))\n",
        "      res.append(self.out(h))\n",
        "    return torch.stack(res, dim=1)\n",
        "\n",
        "\n",
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.i_h = nn.Embedding(nv,nh)\n",
        "        self.h_h = nn.Linear(nh,nh)\n",
        "        self.h_o = nn.Linear(nh,nv)\n",
        "        self.bn = nn.BatchNorm1d(nh)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = torch.zeros(x.shape[0], nh).to(device=x.device)\n",
        "        res = []\n",
        "        for i in range(x.shape[1]):\n",
        "            h = h + self.i_h(x[:,i])\n",
        "            h = F.relu(self.h_h(h))\n",
        "            res.append(self.h_o(self.bn(h)))\n",
        "        #pdb.set_trace()\n",
        "        return torch.stack(res, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlMreM6a5UkS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "35f9a92f-1b6d-4c77-9eff-b33ade6e8ff4"
      },
      "source": [
        "learn = Learner(data, Model2(), metrics=accuracy)\n",
        "learn.fit_one_cycle(10, 3e-3)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.410916</td>\n",
              "      <td>3.198211</td>\n",
              "      <td>0.233026</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.515452</td>\n",
              "      <td>2.168845</td>\n",
              "      <td>0.308949</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.924214</td>\n",
              "      <td>2.302819</td>\n",
              "      <td>0.311932</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.675017</td>\n",
              "      <td>2.314949</td>\n",
              "      <td>0.312074</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.563446</td>\n",
              "      <td>2.288628</td>\n",
              "      <td>0.311648</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.503242</td>\n",
              "      <td>2.179128</td>\n",
              "      <td>0.317827</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.466725</td>\n",
              "      <td>2.196350</td>\n",
              "      <td>0.322159</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.441664</td>\n",
              "      <td>2.264104</td>\n",
              "      <td>0.326847</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.422222</td>\n",
              "      <td>2.300416</td>\n",
              "      <td>0.328764</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.409647</td>\n",
              "      <td>2.295279</td>\n",
              "      <td>0.328480</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-LuZR1FgJy8",
        "colab_type": "text"
      },
      "source": [
        "Note that our accuracy is worse now, because we are doing a harder task.  When we predict word k (k<70), we have less history to help us then when we were only predicting word 71."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54DSp8j5gJy8",
        "colab_type": "text"
      },
      "source": [
        "## Maintain state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShsLEGqrgJy9",
        "colab_type": "text"
      },
      "source": [
        "To address this issue, let's keep the hidden state from the previous line of text, so we are not starting over again on each new line of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKrqKgVZfgvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(nv, wordvec_len)\n",
        "    self.input = nn.Linear(wordvec_len, nh)\n",
        "    self.hid = nn.Linear(nh, nh)\n",
        "    self.out = nn.Linear(nh, nv)\n",
        "    self.bn = nn.BatchNorm1d(nh)\n",
        "    self.h = torch.zeros(bs, nh).cuda()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = self.h\n",
        "    res = []\n",
        "    for i in range(x.shape[1]):\n",
        "      h = h + F.relu(self.input(self.emb(x[:, i])))\n",
        "      h = self.bn(F.relu(self.hid(h)))\n",
        "      res.append(h)\n",
        "    self.h = h.detach()\n",
        "    res = torch.stack(res, dim=1)\n",
        "    return self.out(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYuBa46UguXo",
        "colab_type": "code",
        "outputId": "e0f37c7d-362f-48ea-a6e2-ec6748a88638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "learn = Learner(data, Model3(), metrics=accuracy)\n",
        "learn.fit_one_cycle(20, 3e-3)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.737108</td>\n",
              "      <td>10.886840</td>\n",
              "      <td>0.072727</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.330510</td>\n",
              "      <td>2.784832</td>\n",
              "      <td>0.419957</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.551389</td>\n",
              "      <td>2.021663</td>\n",
              "      <td>0.471023</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.926469</td>\n",
              "      <td>1.872175</td>\n",
              "      <td>0.488423</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.542643</td>\n",
              "      <td>1.757848</td>\n",
              "      <td>0.522585</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.283859</td>\n",
              "      <td>1.702196</td>\n",
              "      <td>0.569460</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.080044</td>\n",
              "      <td>1.602201</td>\n",
              "      <td>0.572514</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.915017</td>\n",
              "      <td>1.656019</td>\n",
              "      <td>0.571875</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.783566</td>\n",
              "      <td>1.476927</td>\n",
              "      <td>0.580185</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.672636</td>\n",
              "      <td>1.378490</td>\n",
              "      <td>0.583168</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.584713</td>\n",
              "      <td>1.263261</td>\n",
              "      <td>0.639986</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.508300</td>\n",
              "      <td>1.191417</td>\n",
              "      <td>0.686364</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.445239</td>\n",
              "      <td>1.172220</td>\n",
              "      <td>0.692614</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.397735</td>\n",
              "      <td>1.181186</td>\n",
              "      <td>0.672940</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.357133</td>\n",
              "      <td>1.108489</td>\n",
              "      <td>0.699148</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.327603</td>\n",
              "      <td>1.054317</td>\n",
              "      <td>0.736861</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.302043</td>\n",
              "      <td>1.070057</td>\n",
              "      <td>0.727770</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.282737</td>\n",
              "      <td>1.118421</td>\n",
              "      <td>0.712926</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.268886</td>\n",
              "      <td>1.081568</td>\n",
              "      <td>0.722514</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.260669</td>\n",
              "      <td>1.081539</td>\n",
              "      <td>0.722159</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PIZXQCnj_zU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(nv, wordvec_len)\n",
        "    self.input = nn.Linear(wordvec_len, nh)\n",
        "    self.hid = nn.Linear(nh, nh)\n",
        "    self.out = nn.Linear(nh, nv)\n",
        "    self.bn = nn.BatchNorm1d(nh)\n",
        "    self.h = torch.zeros(bs, nh).cuda()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = self.h\n",
        "    res = []\n",
        "    for i in range(x.shape[1]):\n",
        "      h = h + torch.tanh(self.input(self.emb(x[:, i])))\n",
        "      h = self.bn(torch.tanh(self.hid(h)))\n",
        "      res.append(h)\n",
        "    self.h = h.detach()\n",
        "    res = torch.stack(res, dim=1)\n",
        "    return self.out(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFbPpTi_kKJl",
        "colab_type": "code",
        "outputId": "40e8532f-5785-4777-f50e-d04bd51606e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "learn = Learner(data, Model3(), metrics=accuracy)\n",
        "learn.fit_one_cycle(20, 3e-3)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.496202</td>\n",
              "      <td>3.190429</td>\n",
              "      <td>0.352912</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.748388</td>\n",
              "      <td>2.198311</td>\n",
              "      <td>0.486364</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.015454</td>\n",
              "      <td>1.744916</td>\n",
              "      <td>0.536719</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.477366</td>\n",
              "      <td>1.397689</td>\n",
              "      <td>0.594531</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.098995</td>\n",
              "      <td>1.234465</td>\n",
              "      <td>0.633381</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.845759</td>\n",
              "      <td>1.164001</td>\n",
              "      <td>0.651989</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.667688</td>\n",
              "      <td>1.052913</td>\n",
              "      <td>0.682102</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.544539</td>\n",
              "      <td>0.988072</td>\n",
              "      <td>0.702699</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.453382</td>\n",
              "      <td>0.934729</td>\n",
              "      <td>0.717756</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.384751</td>\n",
              "      <td>0.934515</td>\n",
              "      <td>0.724432</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.332945</td>\n",
              "      <td>0.861203</td>\n",
              "      <td>0.741832</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.296024</td>\n",
              "      <td>0.872070</td>\n",
              "      <td>0.749858</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.263609</td>\n",
              "      <td>0.853808</td>\n",
              "      <td>0.754261</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.236497</td>\n",
              "      <td>0.813794</td>\n",
              "      <td>0.764418</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.213213</td>\n",
              "      <td>0.823658</td>\n",
              "      <td>0.762713</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.193776</td>\n",
              "      <td>0.834583</td>\n",
              "      <td>0.759517</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.179285</td>\n",
              "      <td>0.845180</td>\n",
              "      <td>0.757031</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.167008</td>\n",
              "      <td>0.859531</td>\n",
              "      <td>0.751491</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.157094</td>\n",
              "      <td>0.860083</td>\n",
              "      <td>0.754972</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.150660</td>\n",
              "      <td>0.854162</td>\n",
              "      <td>0.754474</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jluoFl9_gJzE",
        "colab_type": "text"
      },
      "source": [
        "Now we are getting greater accuracy than before!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ulBMFjZgJzF",
        "colab_type": "text"
      },
      "source": [
        "## nn.RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk5Ny7j8gJzF",
        "colab_type": "text"
      },
      "source": [
        "Let's refactor the above to use PyTorch's RNN.  This is what you would use in practice, but now you know the inside details!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxHxCittkjGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model4(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(nv, wordvec_len)\n",
        "    self.input = nn.Linear(wordvec_len, nh)\n",
        "    self.rnn = nn.RNN(nh, nh, 1, batch_first=True)\n",
        "    self.out = nn.Linear(nh, nv)\n",
        "    self.bn = BatchNorm1dFlat(nh)\n",
        "    self.h = torch.zeros(1, bs, nh).cuda()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    res, h = self.rnn(self.input(self.emb(x)), self.h)\n",
        "    self.h = h.detach()\n",
        "    return self.out(self.bn(res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyOMtw3tjG-i",
        "colab_type": "code",
        "outputId": "58cf19cd-9781-4564-f9f0-dcd94dcf4f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "learn = Learner(data, Model4(), metrics=accuracy)\n",
        "learn.fit_one_cycle(20, 3e-3)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.454730</td>\n",
              "      <td>3.175731</td>\n",
              "      <td>0.348793</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.667047</td>\n",
              "      <td>2.125803</td>\n",
              "      <td>0.464133</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.036313</td>\n",
              "      <td>2.063892</td>\n",
              "      <td>0.316619</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.720699</td>\n",
              "      <td>2.120566</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.544179</td>\n",
              "      <td>1.813197</td>\n",
              "      <td>0.482955</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.364512</td>\n",
              "      <td>1.815946</td>\n",
              "      <td>0.498438</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.149287</td>\n",
              "      <td>1.668347</td>\n",
              "      <td>0.482173</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.940415</td>\n",
              "      <td>1.386655</td>\n",
              "      <td>0.557457</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.764807</td>\n",
              "      <td>1.242821</td>\n",
              "      <td>0.573366</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.630987</td>\n",
              "      <td>1.201925</td>\n",
              "      <td>0.594460</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.525777</td>\n",
              "      <td>1.108968</td>\n",
              "      <td>0.634659</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.446960</td>\n",
              "      <td>1.139579</td>\n",
              "      <td>0.639560</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.388087</td>\n",
              "      <td>1.140157</td>\n",
              "      <td>0.643537</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.342816</td>\n",
              "      <td>1.199414</td>\n",
              "      <td>0.633736</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.308237</td>\n",
              "      <td>1.139524</td>\n",
              "      <td>0.648722</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.283078</td>\n",
              "      <td>1.124835</td>\n",
              "      <td>0.662003</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.261476</td>\n",
              "      <td>1.067836</td>\n",
              "      <td>0.680043</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.244309</td>\n",
              "      <td>1.112445</td>\n",
              "      <td>0.670455</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.233984</td>\n",
              "      <td>1.112467</td>\n",
              "      <td>0.668253</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.226839</td>\n",
              "      <td>1.101279</td>\n",
              "      <td>0.671307</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WHaNCoRw7NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model4(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(nv, wordvec_len)\n",
        "    self.input = nn.Linear(wordvec_len, nh)\n",
        "    self.rnn = nn.RNN(nh, nh, 2, batch_first=True, dropout=0.1)\n",
        "    self.out = nn.Linear(nh, nv)\n",
        "    self.bn = BatchNorm1dFlat(nh)\n",
        "    self.h = torch.zeros(2, bs, nh).cuda()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    res, h = self.rnn(self.input(self.emb(x)), self.h)\n",
        "    self.h = h.detach()\n",
        "    return self.out(self.bn(res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaGqOW32xCXI",
        "colab_type": "code",
        "outputId": "84909a0b-7d3e-446f-d72e-79af8e5a9846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "learn = Learner(data, Model4(), metrics=accuracy)\n",
        "learn.fit_one_cycle(20, 3e-3)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.126027</td>\n",
              "      <td>2.708646</td>\n",
              "      <td>0.455327</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.375827</td>\n",
              "      <td>1.994828</td>\n",
              "      <td>0.468679</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.895900</td>\n",
              "      <td>1.894145</td>\n",
              "      <td>0.367756</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.627582</td>\n",
              "      <td>1.759445</td>\n",
              "      <td>0.476847</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.352829</td>\n",
              "      <td>1.435170</td>\n",
              "      <td>0.521875</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.043024</td>\n",
              "      <td>1.078941</td>\n",
              "      <td>0.655114</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.755228</td>\n",
              "      <td>0.938894</td>\n",
              "      <td>0.694673</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.548171</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.741193</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.412124</td>\n",
              "      <td>0.841281</td>\n",
              "      <td>0.755185</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.320531</td>\n",
              "      <td>0.792354</td>\n",
              "      <td>0.772230</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.255602</td>\n",
              "      <td>0.793126</td>\n",
              "      <td>0.778764</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.211869</td>\n",
              "      <td>0.831388</td>\n",
              "      <td>0.774503</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.181044</td>\n",
              "      <td>0.887395</td>\n",
              "      <td>0.762642</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.155509</td>\n",
              "      <td>0.771132</td>\n",
              "      <td>0.801634</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.137164</td>\n",
              "      <td>0.803544</td>\n",
              "      <td>0.789347</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.123147</td>\n",
              "      <td>0.816822</td>\n",
              "      <td>0.789773</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.111867</td>\n",
              "      <td>0.815223</td>\n",
              "      <td>0.793466</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.102646</td>\n",
              "      <td>0.834602</td>\n",
              "      <td>0.788210</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.096860</td>\n",
              "      <td>0.828752</td>\n",
              "      <td>0.791761</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.092941</td>\n",
              "      <td>0.836145</td>\n",
              "      <td>0.790909</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO4e4G-kqpSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "??BatchNorm1dFlat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9_28v_8gJzK",
        "colab_type": "text"
      },
      "source": [
        "## nn.GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uud6L-ESgJzK",
        "colab_type": "text"
      },
      "source": [
        "When you have long time scales and deeper networks, these become impossible to train.  One way to address this is to add mini-NN to decide how much of the green arrow and how much of the orange arrow to keep.  These mini-NNs can be GRUs or LSTMs.  We will cover more details of this in a later lesson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyM-QyTHe5SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(nv, wordvec_len)\n",
        "    self.input = nn.Linear(wordvec_len, nh)\n",
        "    self.rnn = nn.GRU(nh, nh, 2, batch_first=True, dropout=0.05)\n",
        "    self.out = nn.Linear(nh, nv)\n",
        "    self.bn = BatchNorm1dFlat(nh)\n",
        "    self.h = torch.zeros(2, bs, nh).cuda()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    res, h = self.rnn(self.input(self.emb(x)), self.h)\n",
        "    self.h = h.detach()\n",
        "    return self.out(self.bn(res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn7gzroSfhBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "??nn.GRU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sJlIUxDfAFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c54a5047-1338-44b2-9914-f9805c63d6ed"
      },
      "source": [
        "learn = Learner(data, Model5(), metrics=accuracy)\n",
        "learn.fit_one_cycle(10, 1e-2)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.696175</td>\n",
              "      <td>2.177383</td>\n",
              "      <td>0.476634</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.664812</td>\n",
              "      <td>1.586279</td>\n",
              "      <td>0.603977</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.881934</td>\n",
              "      <td>1.131488</td>\n",
              "      <td>0.782457</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.447780</td>\n",
              "      <td>1.213786</td>\n",
              "      <td>0.818750</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.228962</td>\n",
              "      <td>1.166247</td>\n",
              "      <td>0.834304</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.123571</td>\n",
              "      <td>1.116273</td>\n",
              "      <td>0.833310</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.071810</td>\n",
              "      <td>1.277520</td>\n",
              "      <td>0.835156</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.043868</td>\n",
              "      <td>1.258988</td>\n",
              "      <td>0.838068</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.029036</td>\n",
              "      <td>1.265061</td>\n",
              "      <td>0.839134</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.021350</td>\n",
              "      <td>1.290411</td>\n",
              "      <td>0.838778</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iS4iqWKgJzR",
        "colab_type": "text"
      },
      "source": [
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H77PcYbsgJzR",
        "colab_type": "text"
      },
      "source": [
        "RNNs are just a refactored, fully-connected neural network.\n",
        "\n",
        "You can use the same approach for any sequence labeling task (part of speech, classifying whether material is sensitive,..)"
      ]
    }
  ]
}